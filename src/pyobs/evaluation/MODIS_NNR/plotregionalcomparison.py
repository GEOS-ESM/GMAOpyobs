#!/usr/bin/env python
"""
This code uses the output generated by monthlygeossample_speciated.py to
create figures comparing GEOS with MODIS NNR data. Plots include global maps
of regional summaries (bias, correlation, scaling), and individual figures
showing the annual cycle of total AOD and how that is broken down by species in GEOS.
"""

import numpy as np
import xarray as xr
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.patches as patches
from matplotlib.patches import Rectangle
from matplotlib.colors import TwoSlopeNorm
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from pathlib import Path
import pandas as pd
from datetime import datetime
import seaborn as sns
import argparse
import sys
from scipy.stats import pearsonr

# Define regions (including global)
REGIONS = {
    0: {'name': 'Global', 'lon': [-180, 180], 'lat': [-90, 90]},
    1: {'name': 'Alaska', 'lon': [-170, -140], 'lat': [50, 70]},
    2: {'name': 'Canada', 'lon': [-140, -80], 'lat': [50, 70]},
    3: {'name': 'Quebec', 'lon': [-80, -55], 'lat': [45, 65]},
    4: {'name': 'USWest', 'lon': [-130, -105], 'lat': [30, 50]},
    5: {'name': 'USCentral', 'lon': [-105, -90], 'lat': [30, 50]},
    6: {'name': 'USEast', 'lon': [-90, -70], 'lat': [25, 45]},
    7: {'name': 'Mexico', 'lon': [-120, -85], 'lat': [10, 30]},
    8: {'name': 'BrazilFor', 'lon': [-75, -50], 'lat': [-15, 5]},
    9: {'name': 'BrazilCer', 'lon': [-50, -30], 'lat': [-20, 0]},
    10: {'name': 'Argentina', 'lon': [-75, -50], 'lat': [-60, -15]},
    11: {'name': 'AfricaWest', 'lon': [-20, 15], 'lat': [0, 15]},
    12: {'name': 'AfricaCent', 'lon': [15, 30], 'lat': [5, 15]},
    13: {'name': 'AfricaEast', 'lon': [30, 50], 'lat': [-10, 15]},
    14: {'name': 'Congo', 'lon': [10, 30], 'lat': [-10, 5]},
    15: {'name': 'Zambia', 'lon': [22, 35], 'lat': [-18, -8]},
    16: {'name': 'AfricaSouth', 'lon': [10, 35], 'lat': [-35, -20]},
    17: {'name': 'Madagascar', 'lon': [42, 50], 'lat': [-25, -12]},
    18: {'name': 'Scandinavia', 'lon': [0, 35], 'lat': [55, 75]},
    19: {'name': 'Moscow', 'lon': [30, 60], 'lat': [45, 60]},
    20: {'name': 'SiberiaWest', 'lon': [35, 90], 'lat': [60, 75]},
    21: {'name': 'SiberiaEast', 'lon': [90, 140], 'lat': [60, 75]},
    22: {'name': 'EuropeWest', 'lon': [-10, 30], 'lat': [35, 55]},
    23: {'name': 'MiddleEast', 'lon': [30, 60], 'lat': [30, 45]},
    24: {'name': 'AsiaCent', 'lon': [60, 110], 'lat': [35, 50]},
    25: {'name': 'ChinaEast', 'lon': [110, 150], 'lat': [35, 60]},
    26: {'name': 'Nepal', 'lon': [65, 95], 'lat': [25, 35]},
    27: {'name': 'India', 'lon': [70, 90], 'lat': [5, 25]},
    28: {'name': 'ChinaSouth', 'lon': [100, 125], 'lat': [20, 40]},
    29: {'name': 'Indochina', 'lon': [90, 110], 'lat': [10, 25]},
    30: {'name': 'Philippines', 'lon': [115, 130], 'lat': [5, 20]},
    31: {'name': 'Sumatra', 'lon': [95, 110], 'lat': [-10, 10]},
    32: {'name': 'Borneo', 'lon': [110, 120], 'lat': [-5, 8]},
    33: {'name': 'Indonesia', 'lon': [120, 160], 'lat': [-10, 5]},
    34: {'name': 'AustraliaN', 'lon': [120, 150], 'lat': [-20, -10]},
    35: {'name': 'AustraliaW', 'lon': [110, 130], 'lat': [-35, -20]},
    36: {'name': 'AustraliaE', 'lon': [135, 155], 'lat': [-45, -20]},
    37: {'name': 'SiberiaFE', 'lon': [140, 170], 'lat': [60, 75]},
    38: {'name': 'Sahara', 'lon': [-15, 30], 'lat': [13, 35]},
    39: {'name': 'Sahel', 'lon': [-15, 35], 'lat': [12, 13]},
    40: {'name': 'CapeVerde', 'lon': [-26, -20], 'lat': [10, 20]},
    41: {'name': 'RedSea', 'lon': [30, 45], 'lat': [10, 30]},
    42: {'name': 'PersianGulf', 'lon': [45, 60], 'lat': [20, 30]},
    43: {'name': 'ArabSea', 'lon': [60, 70], 'lat': [10, 20]},
    44: {'name': 'Caribbean', 'lon': [-80, -60], 'lat': [13, 23]},
    45: {'name': 'SALDust', 'lon': [-60, -26], 'lat': [13, 30]},
    46: {'name': 'SAmerBB', 'lon': [-45, -20], 'lat': [-45, -25]}
}

def load_monthly_data(base_path='sampledGEOS/c180R_qfed3igbp_allviirs', sensor='both'):
    """Load all monthly data files and combine into datasets."""
    base_path = Path(base_path)
    sensor_lower = sensor.lower()
    
    # Get all MOD and MYD files
    mod_files = sorted(base_path.glob('*MOD04filtered*.nc4'))
    myd_files = sorted(base_path.glob('*MYD04filtered*.nc4'))
    
    # Filter based on sensor selection
    if sensor_lower in ['terra', 'mod']:
        myd_files = []
        print(f"Using Terra (MOD) data only")
    elif sensor_lower in ['aqua', 'myd']:
        mod_files = []
        print(f"Using Aqua (MYD) data only")
    elif sensor_lower == 'both':
        print(f"Using both Terra (MOD) and Aqua (MYD) data separately")
    else:
        raise ValueError(f"Invalid sensor option: {sensor}. Use 'terra', 'aqua', or 'both'")
    
    print(f"Found {len(mod_files)} MOD files and {len(myd_files)} MYD files")
    
    # Load and combine data
    mod_data = []
    myd_data = []
    
    for f in mod_files:
        try:
            ds = xr.open_dataset(f)
            # Extract month from filename
            month_str = f.name.split('.')[-2]  # e.g., '202401'
            month = int(month_str[-2:])
            ds = ds.assign_coords(month=month)
            mod_data.append(ds)
            print(f"  Loaded Terra month {month}")
        except Exception as e:
            print(f"Error loading {f}: {e}")
    
    for f in myd_files:
        try:
            ds = xr.open_dataset(f)
            month_str = f.name.split('.')[-2]
            month = int(month_str[-2:])
            ds = ds.assign_coords(month=month)
            myd_data.append(ds)
            print(f"  Loaded Aqua month {month}")
        except Exception as e:
            print(f"Error loading {f}: {e}")
    
    # Combine datasets
    if mod_data:
        mod_combined = xr.concat(mod_data, dim='month')
        mod_combined.attrs['sensor'] = 'Terra'
    else:
        mod_combined = None
        
    if myd_data:
        myd_combined = xr.concat(myd_data, dim='month')
        myd_combined.attrs['sensor'] = 'Aqua'
    else:
        myd_combined = None
    
    return mod_combined, myd_combined

def calculate_regional_means(data, region_id):
    """Calculate regional means for a specific region."""
    region = REGIONS[region_id]
    lon_min, lon_max = region['lon']
    lat_min, lat_max = region['lat']
    
    sensor_name = data.attrs.get('sensor', 'MODIS')
    print(f"Calculating means for {region['name']} using {sensor_name}")
    
    if region_id == 0:  # Global region
        region_data = data
        print("  Using global data (no spatial subsetting)")
    else:
        # Handle longitude conversion for regional data
        data_lon = data.lon.values
        if np.any(data_lon > 180):
            # Data is in 0-360 format, convert region bounds
            if lon_min < 0:
                lon_min += 360
            if lon_max < 0:
                lon_max += 360
        
        # Select region
        if lon_min > lon_max:  # Crossing dateline
            region_data = data.where(
                ((data.lon >= lon_min) | (data.lon <= lon_max)) &
                (data.lat >= lat_min) & (data.lat <= lat_max)
            )
        else:
            region_data = data.where(
                (data.lon >= lon_min) & (data.lon <= lon_max) &
                (data.lat >= lat_min) & (data.lat <= lat_max)
            )
    
    # Calculate means
    means = {}
    for var in ['MODtau', 'GEOStau', 'bcexttau', 'ocexttau', 'brexttau', 
                'ssexttau', 'duexttau', 'suexttau', 'niexttau']:
        if var in region_data.data_vars:
            var_mean = region_data[var].mean(dim=['lat', 'lon'], skipna=True)
            means[var] = var_mean
    
    return means

def calculate_regional_statistics(mod_data, myd_data, sensor='both'):
    """Calculate correlation, bias, and scaling factor statistics for all regions."""
    print("Calculating regional statistics...")
    
    stats_data = []
    
    for region_id, region_info in REGIONS.items():
        region_name = region_info['name']
        print(f"  Processing {region_name}...")
        
        # Process each sensor
        sensors_to_process = []
        if sensor.lower() in ['terra', 'both'] and mod_data is not None:
            sensors_to_process.append(('Terra', mod_data))
        if sensor.lower() in ['aqua', 'both'] and myd_data is not None:
            sensors_to_process.append(('Aqua', myd_data))
        
        for sensor_name, data in sensors_to_process:
            try:
                means = calculate_regional_means(data, region_id)
                
                if not means or all(var.isnull().all() for var in means.values()):
                    continue
                
                # Extract valid data
                mod_vals = means['MODtau'].values
                geos_vals = means['GEOStau'].values
                valid_mask = ~(np.isnan(mod_vals) | np.isnan(geos_vals))
                
                if np.sum(valid_mask) < 2:  # Need at least 2 points for correlation
                    continue
                
                valid_mod = mod_vals[valid_mask]
                valid_geos = geos_vals[valid_mask]
                
                # Calculate statistics
                corr, p_value = pearsonr(valid_mod, valid_geos)
                bias = np.mean(valid_geos - valid_mod)
                rmse = np.sqrt(np.mean((valid_geos - valid_mod)**2))
                mean_obs = np.mean(valid_mod)
                mean_model = np.mean(valid_geos)
                n_points = len(valid_mod)
                
                # Calculate scaling factor chi = exp(log(MODtau+0.01) - log(GEOStau+0.01))
                # This is equivalent to (MODtau+0.01)/(GEOStau+0.01)
                mod_adjusted = valid_mod + 0.01
                geos_adjusted = valid_geos + 0.01
                
                # Calculate chi for each time point
                chi_values = np.exp(np.log(mod_adjusted) - np.log(geos_adjusted))
                
                # Calculate statistics of chi
                chi_mean = np.mean(chi_values)
                chi_median = np.median(chi_values)
                chi_std = np.std(chi_values)
                chi_min = np.min(chi_values)
                chi_max = np.max(chi_values)
                
                stats_data.append({
                    'region_id': region_id,
                    'region_name': region_name,
                    'sensor': sensor_name,
                    'correlation': corr,
                    'p_value': p_value,
                    'bias': bias,
                    'rmse': rmse,
                    'mean_obs': mean_obs,
                    'mean_model': mean_model,
                    'n_points': n_points,
                    'chi_mean': chi_mean,
                    'chi_median': chi_median,
                    'chi_std': chi_std,
                    'chi_min': chi_min,
                    'chi_max': chi_max,
                    'lon_min': region_info['lon'][0],
                    'lon_max': region_info['lon'][1],
                    'lat_min': region_info['lat'][0],
                    'lat_max': region_info['lat'][1]
                })
                
            except Exception as e:
                print(f"    Error processing {sensor_name} for {region_name}: {e}")
                continue
    
    return pd.DataFrame(stats_data)

def create_map_plot(stats_df, metric='correlation', sensor='both', vmin=None, vmax=None, 
                   title_suffix='', output_dir='regional_analysis', experiment_name='',
                   show_values=False):
    """Create map visualization of regional statistics with optional value annotations."""
    
    # Filter data by sensor if needed
    if sensor.lower() in ['terra', 'aqua']:
        plot_data = stats_df[stats_df['sensor'].str.lower() == sensor.lower()].copy()
        sensor_title = sensor.capitalize()
    else:
        # For 'both', average the metrics across sensors for each region
        if len(stats_df) == 0:
            print("No data available for mapping")
            return None
        
        # Group by region and average metrics
        numeric_cols = ['correlation', 'bias', 'rmse', 'mean_obs', 'mean_model', 
                       'chi_mean', 'chi_median', 'chi_std', 'chi_min', 'chi_max']
        plot_data = stats_df.groupby('region_id').agg({
            'region_name': 'first',
            'lon_min': 'first', 'lon_max': 'first',
            'lat_min': 'first', 'lat_max': 'first',
            'n_points': 'sum',
            **{col: 'mean' for col in numeric_cols if col in stats_df.columns}
        }).reset_index()
        sensor_title = 'Combined (Terra+Aqua)'
    
    if len(plot_data) == 0:
        print(f"No data available for sensor: {sensor}")
        return None
    
    # Set up the plot
    fig = plt.figure(figsize=(18, 12))  # Slightly larger for text annotations
    ax = plt.axes(projection=ccrs.PlateCarree())
    
    # Add map features
    ax.add_feature(cfeature.COASTLINE, alpha=0.5)
    ax.add_feature(cfeature.BORDERS, alpha=0.3)
    ax.add_feature(cfeature.OCEAN, color='lightblue', alpha=0.3)
    ax.add_feature(cfeature.LAND, color='lightgray', alpha=0.3)
    
    # Set global extent
    ax.set_global()
    
    # Define color mapping based on metric
    if metric == 'correlation':
        if vmin is None or vmax is None:
            vmin, vmax = -1, 1
        cmap = plt.cm.RdBu_r  # Red for negative, blue for positive
        cmap_label = 'Correlation Coefficient'
        title_metric = 'Correlation'
    elif metric == 'bias':
        if vmin is None or vmax is None:
            abs_max = max(abs(plot_data[metric].min()), abs(plot_data[metric].max()))
            vmin, vmax = -abs_max, abs_max
        cmap = plt.cm.RdBu  # Blue for negative bias, red for positive
        cmap_label = 'Bias (GEOS - MODIS)'
        title_metric = 'Bias'
    elif metric == 'rmse':
        if vmin is None or vmax is None:
            vmin, vmax = 0, plot_data[metric].max()
        cmap = plt.cm.Reds  # White to red for RMSE
        cmap_label = 'Root Mean Square Error'
        title_metric = 'RMSE'
    elif metric == 'chi_mean':
        if vmin is None or vmax is None:
            vmin, vmax = 0.5, 2.0  # Reasonable default range for scaling factors
        cmap = plt.cm.RdYlBu_r  # Red for high scaling, blue for low scaling
        cmap_label = 'Scaling Factor (χ)'
        title_metric = 'Scaling Factor (χ)'
        show_values = True  # Always show values for chi
    else:
        # Default settings
        if vmin is None or vmax is None:
            vmin, vmax = plot_data[metric].min(), plot_data[metric].max()
        cmap = plt.cm.viridis
        cmap_label = metric.replace('_', ' ').title()
        title_metric = metric.replace('_', ' ').title()
    
    # Create color normalization
    norm = plt.Normalize(vmin=vmin, vmax=vmax)
    
    # Plot regions with different styles for better visibility
    for idx, row in plot_data.iterrows():
        # Skip global region (region_id = 0) for mapping
        if row['region_id'] == 0:
            continue
            
        lon_min, lon_max = row['lon_min'], row['lon_max']
        lat_min, lat_max = row['lat_min'], row['lat_max']
        
        # Handle longitude wrapping
        if lon_min > lon_max:  # Crosses dateline
            # Split into two rectangles
            rect1 = Rectangle((lon_min, lat_min), 180 - lon_min, lat_max - lat_min,
                            transform=ccrs.PlateCarree(), alpha=0.7, 
                            edgecolor='black', linewidth=1.5)
            rect2 = Rectangle((-180, lat_min), lon_max + 180, lat_max - lat_min,
                            transform=ccrs.PlateCarree(), alpha=0.7,
                            edgecolor='black', linewidth=1.5)
            
            color = cmap(norm(row[metric]))
            rect1.set_facecolor(color)
            rect2.set_facecolor(color)
            ax.add_patch(rect1)
            ax.add_patch(rect2)
        else:
            # Regular rectangle
            rect = Rectangle((lon_min, lat_min), lon_max - lon_min, lat_max - lat_min,
                           transform=ccrs.PlateCarree(), alpha=0.7,
                           edgecolor='black', linewidth=1.5)
            
            color = cmap(norm(row[metric]))
            rect.set_facecolor(color)
            ax.add_patch(rect)
        
        # Calculate center for text placement
        center_lon = (lon_min + lon_max) / 2
        center_lat = (lat_min + lat_max) / 2
        
        # Adjust for dateline crossing
        if lon_min > lon_max:
            if center_lon < 0:
                center_lon += 180
            else:
                center_lon -= 180
        
        if show_values:
            # Add both region name and metric value
            value_text = f"{row['region_name']}\nχ = {row[metric]:.3f}"
            fontsize = 9
            bbox_props = dict(boxstyle='round,pad=0.4', facecolor='white', 
                            alpha=0.9, edgecolor='black', linewidth=0.5)
        else:
            # Add just region name
            value_text = row['region_name']
            fontsize = 8
            bbox_props = dict(boxstyle='round,pad=0.3', facecolor='white', 
                            alpha=0.8, edgecolor='none')
        
        ax.text(center_lon, center_lat, value_text, 
               transform=ccrs.PlateCarree(),
               ha='center', va='center', fontsize=fontsize, fontweight='bold',
               bbox=bbox_props)
    
    # Add colorbar
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])
    cbar = plt.colorbar(sm, ax=ax, shrink=0.6, aspect=20, pad=0.02)
    cbar.set_label(cmap_label, fontsize=12)
    
    # Add gridlines
    gl = ax.gridlines(draw_labels=True, alpha=0.5)
    gl.top_labels = False
    gl.right_labels = False
    
    # Set title
    title = f'{title_metric} - {sensor_title}'
    if title_suffix:
        title += f' - {title_suffix}'
    if experiment_name:
        title += f' ({experiment_name})'
    
    plt.title(title, fontsize=14, fontweight='bold', pad=20)
    
    # Save the plot
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    if show_values and metric == 'chi_mean':
        filename = f"{experiment_name}_{metric}_{sensor.lower()}_map_with_values.png"
    else:
        filename = f"{experiment_name}_{metric}_{sensor.lower()}_map.png"
    
    save_path = output_dir / filename
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"Map saved to: {save_path}")
    
    return fig, save_path

def create_chi_value_map(stats_df, sensor='both', output_dir='regional_analysis', 
                        experiment_name='', title_suffix=''):
    """Create a dedicated map showing chi values with annotations."""
    
    print("Creating chi scaling factor map with values...")
    
    # Filter data by sensor if needed
    if sensor.lower() in ['terra', 'aqua']:
        plot_data = stats_df[stats_df['sensor'].str.lower() == sensor.lower()].copy()
        sensor_title = sensor.capitalize()
    else:
        # For 'both', average the metrics across sensors for each region
        if len(stats_df) == 0:
            print("No data available for mapping")
            return None
        
        # Group by region and average chi values
        numeric_cols = ['chi_mean', 'chi_median', 'chi_std', 'chi_min', 'chi_max']
        plot_data = stats_df.groupby('region_id').agg({
            'region_name': 'first',
            'lon_min': 'first', 'lon_max': 'first',
            'lat_min': 'first', 'lat_max': 'first',
            'n_points': 'sum',
            **{col: 'mean' for col in numeric_cols if col in stats_df.columns}
        }).reset_index()
        sensor_title = 'Combined (Terra+Aqua)'
    
    if len(plot_data) == 0 or 'chi_mean' not in plot_data.columns:
        print(f"No chi data available for sensor: {sensor}")
        return None
    
    # Set up the plot with larger size for better text visibility
    fig = plt.figure(figsize=(20, 14))
    ax = plt.axes(projection=ccrs.PlateCarree())
    
    # Add map features
    ax.add_feature(cfeature.COASTLINE, alpha=0.5)
    ax.add_feature(cfeature.BORDERS, alpha=0.3)
    ax.add_feature(cfeature.OCEAN, color='lightblue', alpha=0.3)
    ax.add_feature(cfeature.LAND, color='lightgray', alpha=0.3)
    
    # Set global extent
    ax.set_global()
    
    # Define chi-specific color mapping
    chi_min = plot_data['chi_mean'].min()
    chi_max = plot_data['chi_mean'].max()
    
    # Set reasonable limits for chi visualization
    vmin = max(0.2, chi_min * 0.9)  # Don't go below 0.2
    vmax = min(3.0, chi_max * 1.1)  # Don't go above 3.0
    
    # Use a diverging colormap centered at 1.0 (perfect scaling)
    norm = TwoSlopeNorm(vmin=vmin, vcenter=1.0, vmax=vmax)
    cmap = plt.cm.RdYlBu_r  # Red for overestimate (chi > 1), blue for underestimate (chi < 1)
    
    # Plot regions
    for idx, row in plot_data.iterrows():
        # Skip global region for mapping
        if row['region_id'] == 0:
            continue
            
        lon_min, lon_max = row['lon_min'], row['lon_max']
        lat_min, lat_max = row['lat_min'], row['lat_max']
        
        # Check if chi_mean is valid, skip region if not
        if 'chi_mean' not in row or pd.isna(row['chi_mean']):
            print(f"Warning: No valid chi value for region {row['region_name']}")
            continue
        
        chi_value = row['chi_mean']
        
        # Handle longitude wrapping
        if lon_min > lon_max:  # Crosses dateline
            # Split into two rectangles
            rect1 = Rectangle((lon_min, lat_min), 180 - lon_min, lat_max - lat_min,
                            transform=ccrs.PlateCarree(), alpha=0.8, 
                            edgecolor='black', linewidth=2)
            rect2 = Rectangle((-180, lat_min), lon_max + 180, lat_max - lat_min,
                            transform=ccrs.PlateCarree(), alpha=0.8,
                            edgecolor='black', linewidth=2)
            
            color = cmap(norm(chi_value))
            rect1.set_facecolor(color)
            rect2.set_facecolor(color)
            ax.add_patch(rect1)
            ax.add_patch(rect2)
        else:
            # Regular rectangle
            rect = Rectangle((lon_min, lat_min), lon_max - lon_min, lat_max - lat_min,
                           transform=ccrs.PlateCarree(), alpha=0.8,
                           edgecolor='black', linewidth=2)
            
            color = cmap(norm(chi_value))
            rect.set_facecolor(color)
            ax.add_patch(rect)
        
        # Calculate center for text placement
        center_lon = (lon_min + lon_max) / 2
        center_lat = (lat_min + lat_max) / 2
        
        # Adjust for dateline crossing
        if lon_min > lon_max:
            if center_lon < 0:
                center_lon += 180
            else:
                center_lon -= 180
        
        # Create text with region name and chi value
        # Ensure chi value is properly formatted to handle extreme values
        chi_text = f"{row['region_name']}\nχ = {chi_value:.3f}"
        
        # Choose text color based on background
        if chi_value > 1.5:  # Red background
            text_color = 'white'
        elif chi_value < 0.7:  # Blue background
            text_color = 'white'
        else:  # Light background
            text_color = 'black'
        
        ax.text(center_lon, center_lat, chi_text, 
               transform=ccrs.PlateCarree(),
               ha='center', va='center', fontsize=10, fontweight='bold',
               color=text_color,
               bbox=dict(boxstyle='round,pad=0.4', facecolor='white', 
                        alpha=0.9, edgecolor='black', linewidth=0.8))
    
    # Add colorbar with custom ticks
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])
    cbar = plt.colorbar(sm, ax=ax, shrink=0.7, aspect=25, pad=0.02)
    
    # Use the exact requested formula in the colorbar label
    cbar.set_label('Scaling Factor (χ = exp(log(MODtau+0.01)-log(GEOStau+0.01)))', 
                  fontsize=12, fontweight='bold')
    
    # Add interpretation text to colorbar
    cbar.ax.text(1.15, 1.02, 'GEOS underestimates', transform=cbar.ax.transAxes, 
                rotation=0, ha='left', va='bottom', fontsize=10, color='red', fontweight='bold')
    cbar.ax.text(1.15, -0.02, 'GEOS overestimates', transform=cbar.ax.transAxes, 
                rotation=0, ha='left', va='top', fontsize=10, color='blue', fontweight='bold')
    
    # Add reference line at chi=1
    cbar.ax.axhline(y=norm(1.0), color='black', linestyle='--', linewidth=2, alpha=0.7)
    cbar.ax.text(1.05, norm(1.0), 'Perfect match (χ=1)', transform=cbar.ax.get_yaxis_transform(), 
                ha='left', va='center', fontsize=10, fontweight='bold')
    
    # Add gridlines
    gl = ax.gridlines(draw_labels=True, alpha=0.5)
    gl.top_labels = False
    gl.right_labels = False
    
    # Set title
    title = f'Scaling Factor (χ) - {sensor_title}'
    if title_suffix:
        title += f' - {title_suffix}'
    if experiment_name:
        title += f' ({experiment_name})'
    
    plt.title(title, fontsize=16, fontweight='bold', pad=25)
    
    # Add subtitle with interpretation
    plt.figtext(0.5, 0.02, 'χ > 1: GEOS underestimates AOD relative to MODIS  |  χ < 1: GEOS overestimates AOD relative to MODIS', 
                ha='center', va='bottom', fontsize=12, style='italic')
    
    # Save the plot
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    filename = f"{experiment_name}_chi_scaling_factor_{sensor.lower()}_map_with_values.png"
    save_path = output_dir / filename
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"Chi scaling factor map with values saved to: {save_path}")
    
    return fig, save_path

def create_statistical_summary_table(stats_df, output_dir='regional_analysis', experiment_name=''):
    """Create a comprehensive statistical summary table including scaling factors."""
    
    if len(stats_df) == 0:
        print("No data available for summary table")
        return None
    
    # Create output directory
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # Prepare data for table
    summary_data = []
    
    for sensor in stats_df['sensor'].unique():
        sensor_data = stats_df[stats_df['sensor'] == sensor].copy()
        
        for idx, row in sensor_data.iterrows():
            summary_data.append({
                'Region ID': row['region_id'],
                'Region Name': row['region_name'],
                'Sensor': row['sensor'],
                'Correlation': f"{row['correlation']:.3f}",
                'P-value': f"{row['p_value']:.4f}" if not np.isnan(row['p_value']) else 'N/A',
                'Bias': f"{row['bias']:.4f}",
                'RMSE': f"{row['rmse']:.4f}",
                'Mean MODIS': f"{row['mean_obs']:.4f}",
                'Mean GEOS': f"{row['mean_model']:.4f}",
                'Chi Mean': f"{row['chi_mean']:.4f}",
                'Chi Median': f"{row['chi_median']:.4f}",
                'Chi Std': f"{row['chi_std']:.4f}",
                'Chi Min': f"{row['chi_min']:.4f}",
                'Chi Max': f"{row['chi_max']:.4f}",
                'N Points': int(row['n_points'])
            })
    
    # Convert to DataFrame and save
    summary_df = pd.DataFrame(summary_data)
    
    # Save as CSV
    csv_path = output_dir / f"{experiment_name}_regional_statistics_with_chi.csv"
    summary_df.to_csv(csv_path, index=False)
    print(f"Statistical summary with scaling factors saved to: {csv_path}")
    
    # Create a separate chi-focused table
    chi_data = []
    for sensor in stats_df['sensor'].unique():
        sensor_data = stats_df[stats_df['sensor'] == sensor].copy()
        
        for idx, row in sensor_data.iterrows():
            chi_data.append({
                'Region ID': row['region_id'],
                'Region Name': row['region_name'],
                'Sensor': row['sensor'],
                'Chi Mean': f"{row['chi_mean']:.4f}",
                'Chi Median': f"{row['chi_median']:.4f}",
                'Chi Std': f"{row['chi_std']:.4f}",
                'Chi Range': f"[{row['chi_min']:.4f}, {row['chi_max']:.4f}]",
                'N Points': int(row['n_points'])
            })
    
    chi_df = pd.DataFrame(chi_data)
    chi_csv_path = output_dir / f"{experiment_name}_scaling_factors_chi.csv"
    chi_df.to_csv(chi_csv_path, index=False)
    print(f"Scaling factor (chi) table saved to: {chi_csv_path}")
    
    # Create a formatted version for display
    print("\n" + "="*120)
    print("REGIONAL STATISTICS SUMMARY WITH SCALING FACTORS")
    print("="*120)
    print(summary_df.to_string(index=False))
    
    print("\n" + "="*80)
    print("SCALING FACTOR (CHI) SUMMARY")
    print("="*80)
    print(chi_df.to_string(index=False))
    
    return summary_df, csv_path, chi_df, chi_csv_path

def create_all_maps(mod_data, myd_data, sensor='both', output_dir='regional_analysis', 
                   experiment_name='', title_suffix=''):
    """Create maps for all key metrics including chi values."""
    
    # Calculate statistics
    stats_df = calculate_regional_statistics(mod_data, myd_data, sensor=sensor)
    
    if len(stats_df) == 0:
        print("No statistics calculated - no valid data found")
        return None, None
    
    # Create statistical summary
    summary_result = create_statistical_summary_table(stats_df, output_dir, experiment_name)
    
    # Handle both old and new return formats
    if len(summary_result) == 4:
        summary_df, csv_path, chi_df, chi_csv_path = summary_result
    else:
        summary_df, csv_path = summary_result
    
    # Create maps for different metrics
    metrics = ['correlation', 'bias', 'rmse', 'chi_mean']
    figures = {}
    
    for metric in metrics:
        if metric in stats_df.columns:
            print(f"\nCreating {metric} map...")
            try:
                fig, save_path = create_map_plot(stats_df, metric=metric, sensor=sensor,
                                               title_suffix=title_suffix, 
                                               output_dir=output_dir,
                                               experiment_name=experiment_name)
                if fig:
                    figures[metric] = (fig, save_path)
                    plt.close(fig)  # Close to save memory
            except Exception as e:
                print(f"Error creating {metric} map: {e}")
    
    # Create special chi value map with annotations
    if 'chi_mean' in stats_df.columns:
        print(f"\nCreating detailed chi scaling factor map with values...")
        try:
            chi_fig, chi_save_path = create_chi_value_map(stats_df, sensor=sensor,
                                                         output_dir=output_dir,
                                                         experiment_name=experiment_name,
                                                         title_suffix=title_suffix)
            if chi_fig:
                figures['chi_detailed'] = (chi_fig, chi_save_path)
                plt.close(chi_fig)
        except Exception as e:
            print(f"Error creating detailed chi map: {e}")
    
    return figures, stats_df

def plot_single_sensor_panels(data, region_id, axes, sensor_name, shared_limits=None, shared_bc_max=None, shared_y_limits=None, show_legend=True, legend_position='upper left'):
    """Plot time series and scatter plot for a single sensor with shared scaling."""
    means = calculate_regional_means(data, region_id)
    
    if not means or all(var.isnull().all() for var in means.values()):
        axes[0].text(0.5, 0.5, f'No valid data for {sensor_name}', 
                    transform=axes[0].transAxes, ha='center', va='center', fontsize=12)
        axes[1].text(0.5, 0.5, f'No valid data for {sensor_name}', 
                    transform=axes[1].transAxes, ha='center', va='center', fontsize=12)
        return False, None, None, None
    
    # Extract data
    months = data.month.values
    mod_vals = means['MODtau'].values
    geos_vals = means['GEOStau'].values
    
    # Aerosol components with BETTER color separation
    components = {
        'Black Carbon': means.get('bcexttau', xr.DataArray(np.zeros_like(months))).values,
        'Organic Carbon': means.get('ocexttau', xr.DataArray(np.zeros_like(months))).values,
        'Brown Carbon': means.get('brexttau', xr.DataArray(np.zeros_like(months))).values,
        'Sea Salt': means.get('ssexttau', xr.DataArray(np.zeros_like(months))).values,
        'Dust': means.get('duexttau', xr.DataArray(np.zeros_like(months))).values,
        'Sulfate': means.get('suexttau', xr.DataArray(np.zeros_like(months))).values,
        'Nitrate': means.get('niexttau', xr.DataArray(np.zeros_like(months))).values
    }
    
    valid_mask = ~(np.isnan(mod_vals) | np.isnan(geos_vals))
    
    if np.sum(valid_mask) > 0:
        valid_months = months[valid_mask]
        valid_mod = mod_vals[valid_mask]
        valid_geos = geos_vals[valid_mask]
        
        # Time series with BETTER color separation
        bottom = np.zeros(len(valid_months))
        component_colors = {
            'Black Carbon': '#2C2C2C',       # Dark gray/black
            'Organic Carbon': '#228B22',     # Forest green (changed from brown)
            'Brown Carbon': '#8B4513',       # Saddle brown
            'Sea Salt': '#4682B4',           # Steel blue
            'Dust': '#DAA520',               # Goldenrod
            'Sulfate': '#FF6347',            # Tomato red
            'Nitrate': '#9370DB'             # Medium purple
        }
        
        # Store the middle y-position of each component for labeling
        component_middles = {}
        
        for comp_name, comp_vals in components.items():
            valid_comp = comp_vals[valid_mask]
            valid_comp = np.nan_to_num(valid_comp, nan=0.0)
            if np.any(valid_comp > 0):
                axes[0].fill_between(valid_months, bottom, bottom + valid_comp, 
                                   alpha=0.8, color=component_colors[comp_name], 
                                   edgecolor='white', linewidth=0.5)
                # Calculate middle position for this component
                component_middles[comp_name] = np.mean(bottom + valid_comp/2)
                bottom += valid_comp
        
        axes[0].plot(valid_months, valid_mod, 'ko-', linewidth=2, markersize=6, 
                    label=f'MODIS AOD ({sensor_name})', zorder=10)
        axes[0].plot(valid_months, valid_geos, 'r--', linewidth=2, 
                    label='GEOS Total AOD', zorder=9)
        
        # Set shared y-axis limits for time series if provided
        if shared_y_limits is not None:
            axes[0].set_ylim(shared_y_limits)
        
        axes[0].set_xlabel('Month')
        axes[0].set_ylabel('AOD')
        axes[0].set_title(f'{sensor_name} - AOD Time Series')
        axes[0].grid(True, alpha=0.3)
        axes[0].set_xticks(valid_months)
        
        # Add legend conditionally and with specified position
        if show_legend:
            axes[0].legend(loc=legend_position, frameon=True, fancybox=True, shadow=True)
        
        # Add component labels directly on the plot
        x_center = np.mean(valid_months)  # Center x-position
        for comp_name, y_middle in component_middles.items():
            # Only label if the component has significant contribution
            if y_middle > 0.01:  # Only label components with meaningful contribution
                axes[0].text(x_center, y_middle, comp_name, 
                           ha='center', va='center', fontsize=10, 
                           fontweight='bold', color='white',
                           bbox=dict(boxstyle='round,pad=0.3', 
                                   facecolor='black', alpha=0.7, edgecolor='none'))
        
        # Scatter plot with SHARED sizing and limits
        bc_fraction = means.get('brexttau', xr.DataArray(np.zeros_like(months))).values / np.maximum(means['GEOStau'].values, 1e-10)
        bc_fraction = np.nan_to_num(bc_fraction, nan=0.0)
        
        # Use shared brown carbon maximum for consistent sizing
        min_size = 30
        max_size = 200
        bc_frac_valid = bc_fraction[valid_mask]
        
        if shared_bc_max is not None and shared_bc_max > 0:
            sizes = min_size + (bc_frac_valid / shared_bc_max) * (max_size - min_size)
        elif np.max(bc_frac_valid) > 0:
            sizes = min_size + (bc_frac_valid / np.max(bc_frac_valid)) * (max_size - min_size)
        else:
            sizes = np.full(len(bc_frac_valid), min_size)
        
        scatter = axes[1].scatter(valid_mod, valid_geos, c=valid_months, s=sizes, 
                                alpha=0.7, cmap='viridis', edgecolors='black', linewidth=0.5)
        
        # Use shared axis limits if provided with EXPANDED RANGE (scatter plot only)
        if shared_limits is not None:
            # EXPAND the axis limits by 10% for scatter plot only
            range_val = shared_limits[1] - shared_limits[0]
            expanded_limits = [shared_limits[0] - range_val * 0.1, 
                             shared_limits[1] + range_val * 0.1]
            axes[1].set_xlim(expanded_limits)
            axes[1].set_ylim(expanded_limits)
            
            # 1:1 line using expanded limits
            axes[1].plot(expanded_limits, expanded_limits, 'k--', alpha=0.5, label='1:1 line')
        else:
            # Calculate expanded limits for this panel
            max_val = max(np.max(valid_mod), np.max(valid_geos))
            min_val = min(np.min(valid_mod), np.min(valid_geos))
            range_val = max_val - min_val
            expanded_limits = [min_val - range_val * 0.1, max_val + range_val * 0.1]
            axes[1].set_xlim(expanded_limits)
            axes[1].set_ylim(expanded_limits)
            axes[1].plot(expanded_limits, expanded_limits, 'k--', alpha=0.5, label='1:1 line')
        
        axes[1].set_xlabel(f'MODIS AOD ({sensor_name})')
        axes[1].set_ylabel('GEOS AOD')
        axes[1].set_title(f'{sensor_name} - MODIS vs GEOS AOD')
        axes[1].grid(True, alpha=0.3)
        axes[1].legend()
        
        # Size legend for brown carbon fraction (using shared scale)
        bc_max_for_legend = shared_bc_max if shared_bc_max is not None else np.max(bc_frac_valid)
        if bc_max_for_legend > 0:
            size_legend_values = [0, bc_max_for_legend * 0.5, bc_max_for_legend]
            size_legend_sizes = [min_size, (min_size + max_size) / 2, max_size]
            size_legend_labels = [f'{val:.3f}' for val in size_legend_values]
            
            legend_elements = []
            for size, label in zip(size_legend_sizes, size_legend_labels):
                legend_elements.append(plt.scatter([], [], s=size, c='gray', alpha=0.7, 
                                                 edgecolors='black', linewidth=0.5))
            
            size_legend = axes[1].legend(legend_elements, size_legend_labels, 
                                       title='Brown Carbon\nFraction', 
                                       loc='upper left', bbox_to_anchor=(0.02, 0.98),
                                       frameon=True, fancybox=True, shadow=True)
            axes[1].add_artist(size_legend)
        
        # Calculate and display chi scaling factor on the scatter plot
        valid_mod_adj = valid_mod + 0.01
        valid_geos_adj = valid_geos + 0.01
        chi_values = np.exp(np.log(valid_mod_adj) - np.log(valid_geos_adj))
        chi_mean = np.mean(chi_values)
        
        # Statistics
        try:
            from scipy.stats import pearsonr
            if len(valid_mod) > 1:
                corr, _ = pearsonr(valid_mod, valid_geos)
                bias = np.mean(valid_geos - valid_mod)
                rmse = np.sqrt(np.mean((valid_geos - valid_mod)**2))
                
                stats_text = f'R = {corr:.3f}\nBias = {bias:.3f}\nRMSE = {rmse:.3f}\nχ = {chi_mean:.3f}\nN = {len(valid_mod)}'
                axes[1].text(0.98, 0.02, stats_text, transform=axes[1].transAxes, 
                           verticalalignment='bottom', horizontalalignment='right',
                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        except ImportError:
            pass
        
        # Calculate y-axis limits for time series (including stacked components)
        # Find maximum values including stacked components
        total_vals = np.zeros_like(valid_geos)
        for comp_vals in components.values():
            valid_comp = comp_vals[valid_mask]
            valid_comp = np.nan_to_num(valid_comp, nan=0.0)
            total_vals += valid_comp
        
        # Include MODIS and GEOS values in y-range calculation
        all_y_values = np.concatenate([valid_mod, valid_geos, total_vals])
        current_y_limits = [0, np.max(all_y_values) * 1.05]  # Start from 0, add 5% padding at top
        
        # Return original data limits (not expanded) for proper shared scaling calculation
        current_limits = [min(np.min(valid_mod), np.min(valid_geos)), 
                         max(np.max(valid_mod), np.max(valid_geos))]
        current_bc_max = np.max(bc_frac_valid) if len(bc_frac_valid) > 0 else 0
        
        return True, current_limits, current_bc_max, current_y_limits
    else:
        axes[0].text(0.5, 0.5, f'No valid data for {sensor_name}', 
                    transform=axes[0].transAxes, ha='center', va='center', fontsize=12)
        axes[1].text(0.5, 0.5, f'No valid data for {sensor_name}', 
                    transform=axes[1].transAxes, ha='center', va='center', fontsize=12)
        return False, None, None, None

def create_and_save_plot(mod_data, myd_data, region_id, sensor='both', output_dir='regional_analysis', experiment_name='c180R_qfed3igbp_allviirs'):
    """Create and automatically save the analysis plot."""
    region = REGIONS[region_id]
    region_name = region['name']
    
    # Create output directory
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    print(f"Creating plot for {region_name}...")
    
    has_mod = mod_data is not None
    has_myd = myd_data is not None
    
    if sensor.lower() == 'both' and has_mod and has_myd:
        # 4-panel figure with IMPROVED layout and panel labels
        fig = plt.figure(figsize=(20, 14))  # Increased width
        
        # Better layout: 2 rows, 2 real columns with uniform spacing
        # Time series and scatter plot are treated as main columns
        # Colorbar will be added later to use proper positioning
        gs = fig.add_gridspec(2, 2, width_ratios=[1.2, 1], height_ratios=[1, 1],
                             left=0.06, right=0.9, top=0.90, bottom=0.08, 
                             hspace=0.3, wspace=0.25)  # Proper spacing between main columns
        
        # Create main axes
        ax_terra_ts = fig.add_subplot(gs[0, 0])
        ax_terra_scatter = fig.add_subplot(gs[0, 1])
        ax_aqua_ts = fig.add_subplot(gs[1, 0])
        ax_aqua_scatter = fig.add_subplot(gs[1, 1])
        
        axes_terra = [ax_terra_ts, ax_terra_scatter]
        axes_aqua = [ax_aqua_ts, ax_aqua_scatter]
        
        # First pass: get data ranges for shared scaling (without labels)
        terra_success, terra_limits, terra_bc_max, terra_y_limits = plot_single_sensor_panels(
            mod_data, region_id, axes_terra, 'Terra')
        aqua_success, aqua_limits, aqua_bc_max, aqua_y_limits = plot_single_sensor_panels(
            myd_data, region_id, axes_aqua, 'Aqua')
        
        # Calculate shared limits and brown carbon maximum
        shared_limits = None
        shared_bc_max = None
        shared_y_limits = None
        
        if terra_success and aqua_success:
            if terra_limits and aqua_limits:
                shared_limits = [min(terra_limits[0], aqua_limits[0]), 
                               max(terra_limits[1], aqua_limits[1])]
            if terra_bc_max and aqua_bc_max:
                shared_bc_max = max(terra_bc_max, aqua_bc_max)
            if terra_y_limits and aqua_y_limits:
                shared_y_limits = [min(terra_y_limits[0], aqua_y_limits[0]),
                                 max(terra_y_limits[1], aqua_y_limits[1])]
        elif terra_success and terra_limits:
            shared_limits = terra_limits
            shared_bc_max = terra_bc_max
            shared_y_limits = terra_y_limits
        elif aqua_success and aqua_limits:
            shared_limits = aqua_limits
            shared_bc_max = aqua_bc_max
            shared_y_limits = aqua_y_limits
        
        # Second pass: plot with shared scaling
        if terra_success:
            ax_terra_ts.clear()
            ax_terra_scatter.clear()
            
            plot_single_sensor_panels(mod_data, region_id, axes_terra, 'Terra', 
                                     shared_limits, shared_bc_max, shared_y_limits,
                                     show_legend=True, legend_position='upper left')
        
        if aqua_success:
            ax_aqua_ts.clear()
            ax_aqua_scatter.clear()
            
            plot_single_sensor_panels(myd_data, region_id, axes_aqua, 'Aqua', 
                                     shared_limits, shared_bc_max, shared_y_limits,
                                     show_legend=False)
        
        # ADD CLEAN PANEL LABELS OUTSIDE the panels (after final plotting)
        ax_terra_ts.text(-0.1, 1.02, 'a)', transform=ax_terra_ts.transAxes, 
                        fontsize=12, fontweight='bold', va='bottom', ha='right')
        ax_terra_scatter.text(-0.1, 1.02, 'b)', transform=ax_terra_scatter.transAxes, 
                             fontsize=12, fontweight='bold', va='bottom', ha='right')
        ax_aqua_ts.text(-0.1, 1.02, 'c)', transform=ax_aqua_ts.transAxes, 
                       fontsize=12, fontweight='bold', va='bottom', ha='right')
        ax_aqua_scatter.text(-0.1, 1.02, 'd)', transform=ax_aqua_scatter.transAxes, 
                            fontsize=12, fontweight='bold', va='bottom', ha='right')
        
        # Add colorbar with proper positioning (minimal gap)
        if aqua_success and len(ax_aqua_scatter.collections) > 0:
            scatter = ax_aqua_scatter.collections[0]
            cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]
            cbar = fig.colorbar(scatter, cax=cbar_ax)
            cbar.set_label('Month', rotation=270, labelpad=15)
        elif terra_success and len(ax_terra_scatter.collections) > 0:
            scatter = ax_terra_scatter.collections[0]
            cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]
            cbar = fig.colorbar(scatter, cax=cbar_ax)
            cbar.set_label('Month', rotation=270, labelpad=15)
        
        fig.suptitle(f'{region_name} - Terra vs Aqua Comparison ({experiment_name})', fontsize=18)
        sensor_str = 'terra_aqua'
        
    else:
        # 2-panel figure with panel labels
        fig, axes = plt.subplots(2, 1, figsize=(14, 12))
        
        if sensor.lower() in ['terra', 'mod'] or (sensor.lower() == 'both' and has_mod and not has_myd):
            if not has_mod:
                print("No Terra data available")
                return None, None
            success, _, _, _ = plot_single_sensor_panels(mod_data, region_id, axes, 'Terra')
            sensor_name = 'Terra'
            sensor_str = 'terra'
        elif sensor.lower() in ['aqua', 'myd'] or (sensor.lower() == 'both' and has_myd and not has_mod):
            if not has_myd:
                print("No Aqua data available")
                return None, None
            success, _, _, _ = plot_single_sensor_panels(myd_data, region_id, axes, 'Aqua')
            sensor_name = 'Aqua'
            sensor_str = 'aqua'
        else:
            print("No data available")
            return None, None
        
        # ADD CLEAN PANEL LABELS for single sensor (after plotting, outside panels)
        axes[0].text(-0.1, 1.02, 'a)', transform=axes[0].transAxes, 
                    fontsize=12, fontweight='bold', va='bottom', ha='right')
        axes[1].text(-0.1, 1.02, 'b)', transform=axes[1].transAxes, 
                    fontsize=12, fontweight='bold', va='bottom', ha='right')
        
        if success:
            # For single sensor plots, we don't need to remove the legend positioning
            # since it's handled within the plotting function
            if len(axes[1].collections) > 0:
                cbar = plt.colorbar(axes[1].collections[0], ax=axes[1], shrink=0.8, aspect=20)
                cbar.set_label('Month', rotation=270, labelpad=15)
        
        fig.suptitle(f'{region_name} - {sensor_name} Analysis ({experiment_name})', fontsize=18)
        plt.subplots_adjust(left=0.08, right=0.85, top=0.92, bottom=0.08, hspace=0.3)
    
    # Save with experiment name in filename (NO region number)
    filename = f"{experiment_name}_{region_name}_{sensor_str}.png"
    save_path = output_dir / filename
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"Plot saved to: {save_path}")
    
    return fig, save_path

def main():
    """Enhanced main function with mapping capability."""
    parser = argparse.ArgumentParser(description='AOD Regional Analysis Tool')
    
    parser.add_argument('--region', '-r', type=int, 
                        help='Single region ID to analyze. Use --list-regions to see options.')
    parser.add_argument('--list-regions', action='store_true',
                        help='List all available regions and exit')
    parser.add_argument('--sensor', '-s', choices=['terra', 'aqua', 'both'], default='both',
                        help='Which MODIS sensor(s) to use (default: both)')
    parser.add_argument('--data-path', '-d', type=str, 
                        default='sampledGEOS/c180R_qfed3igbp_allviirs',
                        help='Path to data directory')
    parser.add_argument('--output', '-o', type=str, default='regional_analysis',
                        help='Output directory for plots')
    parser.add_argument('--experiment', '-e', type=str, default='c180R_qfed3igbp_allviirs',
                        help='Experiment name for output filenames')
    parser.add_argument('--no-display', action='store_true',
                        help='Do not display plots interactively')
    
    # Add mapping arguments
    parser.add_argument('--create-maps', action='store_true',
                        help='Create regional statistics maps')
    parser.add_argument('--maps-only', action='store_true', 
                        help='Only create maps, skip individual region plots')
    parser.add_argument('--chi-map', action='store_true',
                        help='Create dedicated chi scaling factor map with values')
    
    args = parser.parse_args()
    
    if args.list_regions:
        print("Available regions:")
        print("ID  | Name")
        print("----|" + "-"*30)
        for region_id, region_info in REGIONS.items():
            print(f"{region_id:2d}  | {region_info['name']}")
        return
    
    if args.no_display:
        import matplotlib
        matplotlib.use('Agg')
    
    print(f"AOD Regional Analysis Tool")
    print(f"=" * 50)
    print(f"Experiment: {args.experiment}")
    print(f"Data path: {args.data_path}")
    print(f"Sensor: {args.sensor}")
    print(f"Output directory: {args.output}")
    
    try:
        # Load data
        print(f"\nLoading data...")
        mod_data, myd_data = load_monthly_data(base_path=args.data_path, sensor=args.sensor)
        
        if mod_data is None and myd_data is None:
            print("ERROR: No data loaded!")
            return 1
        
        # Create maps if requested
        if args.create_maps or args.maps_only or args.chi_map:
            print("\nCreating regional statistics maps...")
            figures, stats_df = create_all_maps(mod_data, myd_data, 
                                              sensor=args.sensor,
                                              output_dir=args.output,
                                              experiment_name=args.experiment)
            
            # If only chi map is requested, create it specifically
            if args.chi_map and not args.create_maps:
                if 'chi_mean' in stats_df.columns:
                    print("\nCreating dedicated chi scaling factor map...")
                    chi_fig, chi_save_path = create_chi_value_map(
                        stats_df, sensor=args.sensor,
                        output_dir=args.output,
                        experiment_name=args.experiment)
                    
                    if not args.no_display and chi_fig:
                        plt.figure(chi_fig.number)
                        plt.show()
            elif not args.no_display and figures:
                # Display one of the maps
                correlation_fig = figures.get('correlation')
                if correlation_fig:
                    plt.figure(correlation_fig[0].number)
                    plt.show()
        
        # Skip individual plots if maps-only is specified
        if args.maps_only:
            print(f"\nMaps created! Check {args.output}/ for results.")
            return 0
        
        if args.region is not None:
            # Single region
            if args.region not in REGIONS:
                print(f"ERROR: Invalid region ID {args.region}")
                return 1
            
            fig, save_path = create_and_save_plot(mod_data, myd_data, args.region, 
                                                args.sensor, args.output, args.experiment)
            
            if fig and not args.no_display:
                plt.show()
            
        else:
            # All regions (only if not maps-only)
            if not args.create_maps and not args.chi_map:  # Avoid double processing
                print(f"\nProcessing all regions...")
                for region_id in REGIONS.keys():
                    try:
                        fig, save_path = create_and_save_plot(mod_data, myd_data, region_id, 
                                                            args.sensor, args.output, args.experiment)
                        if fig:
                            plt.close(fig)
                    except Exception as e:
                        print(f"Error processing region {region_id}: {e}")
            
            print(f"\nCompleted! Check {args.output}/ for results.")
        
        return 0
        
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
