"""
   Classes for reading PM2.5 AQS data including IMPROVE files.
"""

import os
import sys
import numpy as np
from datetime import timedelta,datetime as TIME
import pandas as pd
import pytz
from timezonefinder import TimezoneFinder

MISSING = -999.

BAD, MARGINAL, GOOD, BEST = list(range(4))

# legacy here refers to the version of the IMPROVE dataset
# that was used to evaluate MERRA-2
# these data files are no longer generated by EPA
VARS_LEGACY = dict(
       PM25 = ('State_Code','County_Code','Site_ID','Parameter',
               'Sample_Duration','Start_Time','Unit','Date',
               'Sample_Value','Uncertainty'),
       SITES = ('State_Code','County_Code','Site_ID',
                'Latitude','Longitude','Land_Use', 'Location_Setting'),
            )
ALIAS_LEGACY = dict(Sample_Value = 'pm25',
             Uncertainty = 'error')

# new files are obtained directly from the Federal Land Manager Environmental Database
# maintained by CIRA: https://views.cira.colostate.edu/fed/DataFiles/

VARS = dict(
       PM25 = ('SiteCode','FactDate','ParamCode','Units','FactValue','Status','Unc','MDL'),
       SITES = ('SiteID','SiteCode','State','County','AQSCode',
                'Latitude','Longitude','Elevation','LandUseCode', 'LocDesc'),
            )
ALIAS = dict(FactValue = 'pm25',
             FactDate = 'Date',
             Unc = 'error')

#---- 
class IMPROVE(object):
    """Base class for EPA data AQS PM2.5"""

    def __init__ (self,Path,xVars=None,Verbose=False,Legacy=False,site_path=None):
        """
        Base class for generic EPA AQS dataset.
        Path: str or list of str, file paths to read
        xVars: dict, optional, defaults to None, variables to read
        Verbose: bool, optional, defaults to False
        Legacy: bool, optional, defaults to False, whether or not to read legacy formated data
        site_path: str, optional, defaults to None, path for location information of IMPROVE sites
        """
        self.Legacy = Legacy
        self.verb = Verbose
        self.columns = None
        self.nobs = 0
        if xVars == None:
            if Legacy:
                self.Vars = VARS_LEGACY['PM25']
                self.ALIAS = ALIAS_LEGACY
            else:
                self.Vars = VARS['PM25']
                self.ALIAS = ALIAS
        else:
            self.Vars = xVars

        if Legacy:
            # Create empty lists for SDS to be read from file
            # -----------------------------------------------
            for name in self.Vars:
                self.__dict__[name] = []

        # Read each granule, appending them to the list
        # ---------------------------------------------
        if type(Path) is list:
            if len(Path) == 0:
                print("WARNING: Empty MxD04_L2 object created")
                return
        else:
                Path = [Path, ]
        self._readList(Path)

        if self.Legacy:
            # Make each attribute a single numpy array
            # ----------------------------------------
            for var in self.Vars:
                try:
                    self.__dict__[var] = np.concatenate(self.__dict__[var])
                except:
                    print("Failed concatenating "+var)


        if self.Legacy:
            # Make aliases
            # ------------
            Alias = list(self.ALIAS.keys())
            for var in self.Vars:
                if var in Alias:
                    self.__dict__[self.ALIAS[var]] = self.__dict__[var]
        else:
            # Make aliases
            # ------------
            self.df.rename(columns=self.ALIAS, inplace=True)
            for var in self.ALIAS.values():
                self.__dict__[var] = self.df[var]
            self.df['Start_Time'] = np.repeat('00:00',self.Date.shape[0])

        # Create timedate
        # ---------------
        self.time = []
        self.tyme = []
        self.julian = np.ones(self.Date.shape[0]).astype('int')
        self.minutes = np.ones(self.Date.shape[0]).astype('int')
        for i in range(self.Date.size):
            date = self.Date[i].replace('-','')
            yy = date[0:4]
            mm = date[4:6]
            dd = date[6:8]
            if Legacy:
                h, m = self.Start_Time[i].split(':')
            else:
                h, m = self.df.Start_Time[i].split(':')
            time = TIME(int(yy),int(mm),int(dd),int(h),int(m),0)
            dt = timedelta(seconds=12*60*60) # add 12 hours
            self.time.append(time+dt)
            self.tyme.append(time)
            self.julian[i] = time.toordinal()
            self.minutes[i] = int(h) * 60 + int(m)

        self.time = np.array(self.time)
        self.tyme = np.array(self.tyme)

        if self.Legacy:
            # Unique list of locations
            # ------------------------
            Locations = {}
            for st in self.State_Code:
                Locations[st] = 1
            self.Stations_st = list(Locations.keys())
        else:
            self.Stations = self.df['SiteCode'].unique()

        # add location data to PM2.5 dataset
        if not self.Legacy and site_path is not None:
            self.site = SITE_MAP(site_path,xVars=xVars,Legacy=Legacy)

            self.df['Longitude'] = 999.
            self.df['Latitude'] = 999.
            self.df['tz_name'] = 'Unknown'
            Start_Time_UTC = []

            # Loop through stations
            for st in self.Stations:
                lat = self.site.df['Latitude'][self.site.df['SiteCode'] == st].values[0]
                lon = self.site.df['Longitude'][self.site.df['SiteCode'] == st].values[0]
                tz  = self.site.df['tz_name'][self.site.df['SiteCode'] == st].values[0]
                mask = self.df['SiteCode'] == st
                self.df.loc[mask, 'Longitude'] = lon
                self.df.loc[mask, 'Latitude'] = lat
                self.df.loc[mask, 'tz_name']  = tz

            # Get measurement Start time in UTC based on measurement location
            for i, start_time in enumerate(self.tyme):
                timezone_name = self.df['tz_name'][i]
                try:
                    local_timezone = pytz.timezone(timezone_name)
                except pytz.UnknownTimeZoneError:
                    print(f"Error: Unknown time zone name '{timezone_name}'")
                    Start_Time_UTC.append(None)
                    continue # go to next ob

                time_aware_local = local_timezone.localize(start_time, is_dst=None)
                time_aware_utc = time_aware_local.astimezone(pytz.utc)
                Start_Time_UTC.append(time_aware_utc)

            self.df['Start_Time_UTC'] = Start_Time_UTC


            # By default all is good if coordinates are ok
            # --------------------------------------------
            #        self.iGood = (abs(self.Longitude)<=180.) & \
            #                     (abs(self.Latitude)<=90.)



#---
    def _readList(self,List):
        """
        Recursively, look for files in list; list items can
        be files or directories.
        """
        for item in List:
            if os.path.isdir(item):      self._readDir(item)
            elif os.path.isfile(item):
                if self.Legacy:
                    self._readEPAdata(item)
                else:
                    self._readIMPROVEdata(item)

            else:
                print("%s is not a valid file or directory, ignoring it"%item)
#---
    def _readDir(self,dir):
        """Recursively, look for files in directory."""
        for item in os.listdir(dir):
            path = dir + os.sep + item
            if os.path.isdir(path):      self._readDir(path)
            elif os.path.isfile(path):
                if self.Legacy:
                    self._readEPAdata(path)
                else:
                    self._readIMPROVEdata(path)
            else:
                print("%s is not a valid file or directory, ignoring it"%item)
#---
    def _readIMPROVEdata(self,filename):
        """Reads IMPROVE data file."""
        if hasattr(self,'df'):
            df = pd.read_table(filename,sep='|')
            self.df = pd.concat([self.df,df],ignore_index=True)
        else:
            self.df = pd.read_table(filename,sep='|')
#---
    def _readEPAdata(self,filename):
        """Reads EPA AQS data file."""

        # Locate Header
        # -------------
        if self.columns == None:
            i = 0
            for line in open(filename).readlines(): 
                if line[0:4] == '# RD':
                    self.columns = line[0:-1].replace('\r','')\
                    .replace(' ','_')\
                    .replace('(','')\
                    .split('|')     #  split the header 
                    self.skip = i + 1 # number of rows to skip for data
                if line[0:4] == '# RC': # header different  
                    self.skip = i + 1 # number of rows to skip for data
                i += 1


            if self.columns == None:
                raise ValueError("Cannot find Column header")

            # Read relevant columns
            # ----------------------------------------
            self.iVars = ()
            self.formats = ()
            self.converters = {}
            for name in self.Vars:
                try:
                    i = self.columns.index(name)
                    
                except:
                    raise ValueError("cannot find <%s> in file"%name)
                self.iVars += (i,)
                if name=='Date':
                    self.formats += ('U8',)
                elif name=='Start_Time':
                    self.formats += ('U5',)
                elif name=='State_Code':
                    self.formats += ('U2',)
                elif name=='County_Code':
                    self.formats += ('U3',)
                elif name=='Site_ID':
                    self.formats += ('U4',)
                else:
                    self.converters[i] = lambda s: float(s or MISSING)
                    self.formats += ('f4',)

#       Read the data
#       -------------
       
        data = np.loadtxt(filename, delimiter='|',
                       dtype={'names':self.Vars,'formats':self.formats},
                       converters = self.converters,
                       skiprows=self.skip, usecols=self.iVars)
        N = len(data)

        self.nobs += N

        # Save data columns as attributes, with nicer names
        # -------------------------------------------------
        for i in range(len(self.Vars)):
            if self.formats[i]=='f4':
                v = np.ones(N)
                for j in range(N):
                    v[j] = data[j][i]
            else:
                v = []
                for j in range(N):
                    v.append(data[j][i])

            self.__dict__[self.Vars[i]] += [v,]

        self.sitemap = {}
        self.sitemap = ((self.State_Code,self.County_Code,self.Site_ID))
        self.sitemap_= list(zip(*self.sitemap))

#---
class SITE_MAP(object):
    """Base class for EPA data AQS lat lon sites"""

    def __init__ (self,Path,xVars=None,Verbose=False,Legacy=False):
        """
        Base class for generic EPA AQS dataset.
        """

        self.verb = Verbose
        self.columns = None
        self.nobs = 0
        self.Legacy = Legacy
        if xVars == None:
            if Legacy:
                self.Vars = VARS_LEGACY['SITES']
                self.ALIAS = ALIAS_LEGACY
            else:
                self.Vars = VARS['SITES']
                self.ALIAS = ALIAS


        if Legacy:
            # Create empty lists for SDS to be read from file
            # -----------------------------------------------
            for name in self.Vars:
                self.__dict__[name] = []

        # Read each granule, appending them to the list
        # ---------------------------------------------
        if type(Path) is list:
            if len(Path) == 0:
                print("WARNING: Empty MxD04_L2 object created")
                return
        else:
                Path = [Path, ]
        self._readList(Path)

        if Legacy:
            # Make each attribute a single numpy array
            # ----------------------------------------
            for var in self.Vars:
                try:
                    self.__dict__[var] = np.concatenate(self.__dict__[var])
                except:
                    print("Failed concatenating "+var)

            # Make aliases
            # ------------
            Alias = list(self.ALIAS.keys())
            for var in self.Vars:
                if var in Alias:
                    self.__dict__[self.ALIAS[var]] = self.__dict__[var]

        else:
            self.Longitude = self.df.Longitude
            self.Latitude  = self.df.Latitude

        # get a UTC offset for each site
        # helpful for converting local times to UTC

        # Get the timezone name from latitude and longitude
        tf = TimezoneFinder()
        tz_name = []
        for lon, lat in zip(self.Longitude, self.Latitude):
            if np.any(np.isnan([lon, lat])):
                tz_name.append('')
            else:
                tz_name.append(tf.timezone_at(lng=lon, lat=lat))

        if Legacy:
            self.tz_name = np.array(tz_name)
        else:
            self.df['tz_name'] = tz_name

#---
    def _readList(self,List):
        """
        Recursively, look for files in list; list items can
        be files or directories.
        """
        for item in List:
            if os.path.isdir(item):      self._readDir(item)
            elif os.path.isfile(item):
                if self.Legacy:
                    self._readLatLon(item)
                else:
                    self._readIMPROVE(item)
            else:
                print("%s is not a valid file or directory, ignoring it"%item)
#---
    def _readDir(self,dir):
        """Recursively, look for files in directory."""
        for item in os.listdir(dir):
            path = dir + os.sep + item
            if os.path.isdir(path):      self._readDir(path)
            elif os.path.isfile(path):
                if self.Legacy:
                    self._readLatLon(path)
                else:
                    self._readIMPROVE(path)
            else:
                print("%s is not a valid file or directory, ignoring it"%item)
#---
    def _readIMPROVE(self,filename):
        """Reads IMPROVE station locations file."""
        if hasattr(self,'df'):
            df = pd.read_table(filename, sep='|')
            self.df = pd.concat([self.df, df], ignore_index=True)
        else:
            self.df = pd.read_table(filename, sep='|')
#---
    def _readLatLon(self,filename):
        """Reads EPA AQS data file."""
        # Locate Header
        # -------------
        if self.columns == None:
            i = 0
            for line in open(filename).readlines(): 
                if line[0:5] == 'State':
                    self.columns = line[0:-1].replace('\r','')\
                    .replace(' ','_')\
                    .split('|')     #  split the header 
                    self.skip = i + 1 # number of rows to skip for data
                i += 1


            if self.columns == None:
                raise ValueError("Cannot find Column header")

            # Read relevant columns
            # ----------------------------------------
            self.iVars = ()
            self.formats = ()
            self.converters = {}
            for name in self.Vars:
                try:
                    i = self.columns.index(name)
                    
                except:
                    raise ValueError("cannot find <%s> in file"%name)
                self.iVars += (i,)
               
                
                if name=='Location_Setting':
                    self.formats += ('S21',)
                elif name=='Land_Use':
                    self.formats += ('S21',)
                elif name=='State_Code':
                    self.formats += ('S2',)
                elif name=='County_Code':
                    self.formats += ('S3',) 
                elif name=='Site_ID':
                    self.formats += ('S4',) 
                else:
                    self.converters[i] = lambda s: float(s or MISSING)
                    self.formats += ('f4',)

#       Read the data
#       -------------
       
        data = np.loadtxt(filename, delimiter='|',
                       dtype={'names':self.Vars,'formats':self.formats},
                       converters = self.converters,
                       skiprows=self.skip, usecols=self.iVars)
        N = len(data)

        self.nobs += N

        # Save data columns as attributes, with nicer names
        # -------------------------------------------------
        for i in range(len(self.Vars)):
            if self.formats[i]=='f4':
                v = np.ones(N)
                for j in range(N):
                    v[j] = data[j][i]
            else:
                v = []
                for j in range(N):
                    v.append(data[j][i])

            self.__dict__[self.Vars[i]] += [v,]


        self.sitemap = {}
        self.sitemap = ((self.State_Code,self.County_Code,self.Site_ID))
        self.sitemap_= list(zip(*self.sitemap))


#..........................................................................

if __name__ == "__main__":

    m = IMPROVE('/nobackup/5/AQS/pm2.5_local/Y2011/RD_501_88101_2011-0.txt')
    g = SITE_MAP('/nobackup/5/SITE_MON_5_13/SITE_MON_5_13_AA_select.txt')

   
